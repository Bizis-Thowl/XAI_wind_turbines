{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/multi/train_hydraulic.csv\", sep=\",\")\n",
    "test = pd.read_csv(\"./data/multi/test_hydraulic.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_target_name = \"Risk Level\"\n",
    "drop_cols = [class_target_name, \"Turbine_ID\", \"Timestamp\", \"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest unbalanced:\\n\")\n",
    "print(test[class_target_name].value_counts())\n",
    "print(\"\\nTrain unbalanced:\")\n",
    "train[class_target_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced = test.drop(test[test[class_target_name] == \"low\"].sample(n=126000).index)\n",
    "train_balanced = train.drop(train[train[class_target_name] == \"low\"].sample(n=211000).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest balanced:\\n\")\n",
    "print(test_balanced[class_target_name].value_counts())\n",
    "print(\"\\nTrain balanced:\")\n",
    "train_balanced[class_target_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillMethod():\n",
    "\n",
    "    def __init__(self, train, test, method) -> None:\n",
    "        self.method = method\n",
    "        if method == \"median\":\n",
    "            filled_train = train.fillna(train.median())\n",
    "            filled_test = test.fillna(test.median())\n",
    "        else:\n",
    "            filled_train = train.fillna(method=method)\n",
    "            filled_test = test.fillna(method=method)\n",
    "        print(filled_train.isna().sum().sum())\n",
    "        print(filled_test.isna().sum().sum())\n",
    "        \n",
    "        # create class balance\n",
    "        self.test = filled_test.drop(filled_test[filled_test[class_target_name] == \"low\"].sample(n=126000).index).sample(frac=1)\n",
    "        self.train = filled_train.drop(filled_train[filled_train[class_target_name] == \"low\"].sample(n=211000).index).sample(frac=1)\n",
    "\n",
    "    def get_train_x(self):\n",
    "        return self.train.drop(columns=drop_cols)\n",
    "\n",
    "    def get_train_y(self):\n",
    "        return self.train[class_target_name]\n",
    "\n",
    "    def get_test_x(self):\n",
    "        return self.test.drop(columns=drop_cols)\n",
    "\n",
    "    def get_test_y(self):\n",
    "        return self.test[class_target_name]\n",
    "\n",
    "    def get_method(self):\n",
    "        return self.method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_methods = [\"median\", \"ffill\", \"bfill\"]\n",
    "fillers = []\n",
    "for method in fill_methods:\n",
    "    fillers.append(FillMethod(train, test, method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if nan-values vanished\n",
    "for filler in fillers:\n",
    "    print(filler.train.isna().sum().sum())\n",
    "    print(filler.test.isna().sum().sum())\n",
    "    print(filler.get_method())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_run(X, y, X_test, y_test, model):\n",
    "\n",
    "    clf = model\n",
    "    clf.fit(X, y)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    y_train_pred = clf.predict(X)\n",
    "    f1_train = f1_score(y, y_train_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    scores = {\n",
    "        \"f1_train\": f1_train,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results[\"rank_test_score\"] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\n",
    "                \"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                    results[\"mean_test_score\"][candidate],\n",
    "                    results[\"std_test_score\"][candidate],\n",
    "                )\n",
    "            )\n",
    "            print(\"Parameters: {0}\".format(results[\"params\"][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_runs(fillers):\n",
    "    \n",
    "    best_scores = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    for filler in fillers:\n",
    "        X_train = filler.get_train_x()\n",
    "        X_test = filler.get_test_x()\n",
    "        y_train = filler.get_train_y()\n",
    "        y_test = filler.get_test_y()\n",
    "\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        y_test = le.fit_transform(y_test)\n",
    "\n",
    "        base_estimator = DecisionTreeClassifier(random_state=0)\n",
    "        # param_grid = {\n",
    "        #     \"max_depth\": [5, 10, 20, 30, 50],\n",
    "        #     \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "        #     \"class_weight\": [\"balanced\", None], \n",
    "        #     \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        # }\n",
    "        param_grid = {\n",
    "            \"max_depth\": [20],\n",
    "        }\n",
    "        sh = GridSearchCV(base_estimator, param_grid, scoring=\"f1_weighted\").fit(X_train, y_train)\n",
    "        clf = sh.best_estimator_\n",
    "        report(sh.cv_results_)\n",
    "\n",
    "        scores = train_run(X_train, y_train, X_test, y_test, model=clf)\n",
    "\n",
    "        best_scores.append({\n",
    "            \"type\": filler.get_method(),  \"f1_train\": scores[\"f1_train\"], \"f1\": scores[\"f1\"],\n",
    "            \"precision\": scores[\"precision\"], \"recall\": scores[\"recall\"], \"clf\": clf})\n",
    "    \n",
    "    return best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run ./utility/model_loader.py -i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores = train_runs(fillers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_scores[0][\"clf\"].predict(fillers[0].get_test_x())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred, bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fillers[0].get_train_y(), bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('XAI_wind')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "520723b62f68f94512b4b527a2129921bc4d6da9c94363ee0894f78e84b35631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

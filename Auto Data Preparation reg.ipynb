{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/first_clean/train_gearbox.csv\", sep=\",\")\n",
    "test = pd.read_csv(\"./data/first_clean/test_gearbox.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train[\"RUL (Target)\"] < 1].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_target_name = \"RUL (Target)\"\n",
    "class_target_name = \"Failure (Target)\"\n",
    "drop_cols = [reg_target_name, class_target_name, \"Turbine_ID\", \"Timestamp\", \"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nans_w_method(train, test, methods: list) -> dict:\n",
    "\n",
    "    method_dict = {\n",
    "        \"X\": {},\n",
    "        \"y\": {},\n",
    "        \"train_timestamps\": {},\n",
    "        \"X_test\": {},\n",
    "        \"y_test\": {},\n",
    "        \"test_timestamps\": {}\n",
    "    }\n",
    "\n",
    "    # train samplen, um Trainings-Zeit zu verk√ºrzen\n",
    "    train_frac = 0.1\n",
    "\n",
    "    for method in methods:\n",
    "        if method == \"median\":\n",
    "            filled_train = train.fillna(train.median()).sample(frac=train_frac)\n",
    "            filled_test = test.fillna(test.median()).sample(frac=1)\n",
    "            method_dict[\"X\"][method] = filled_train.drop(columns=drop_cols)\n",
    "            method_dict[\"y\"][method] = filled_train[reg_target_name]\n",
    "            method_dict[\"train_timestamps\"][method] = filled_train[[\"Turbine_ID\", \"Timestamp\"]]\n",
    "            method_dict[\"X_test\"][method] = filled_test.drop(columns=drop_cols)\n",
    "            method_dict[\"y_test\"][method] = filled_test[reg_target_name]\n",
    "            method_dict[\"test_timestamps\"][method] = filled_test[[\"Turbine_ID\", \"Timestamp\"]]\n",
    "        else:\n",
    "            filled_train = train.fillna(method=method).sample(frac=train_frac)\n",
    "            filled_test = test.fillna(method=method).sample(frac=1)\n",
    "            method_dict[\"X\"][method] = filled_train.drop(columns=drop_cols)\n",
    "            method_dict[\"y\"][method] = filled_train[reg_target_name]\n",
    "            method_dict[\"train_timestamps\"][method] = filled_train[[\"Turbine_ID\", \"Timestamp\"]]\n",
    "            method_dict[\"X_test\"][method] = filled_test.drop(columns=drop_cols)\n",
    "            method_dict[\"y_test\"][method] = filled_test[reg_target_name]\n",
    "            method_dict[\"test_timestamps\"][method] = filled_test[[\"Turbine_ID\", \"Timestamp\"]]\n",
    "\n",
    "    return method_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fill_methods = [\"ffill\", \"bfill\", \"median\"]\n",
    "\n",
    "method_dict = fill_nans_w_method(train, test, fill_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(values):\n",
    "    convert_func = lambda x: 0 if x < 0.99 else 1\n",
    "    converter = np.vectorize(convert_func)\n",
    "    return converter(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_run(X, y, X_test, y_test, clf):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    y_train_pred = clf.predict(X)\n",
    "    discrete_y, discrete_y_pred = discretize(y), discretize(y_train_pred)\n",
    "    discrete_y_test, discrete_y_test_pred = discretize(y_test), discretize(y_test_pred)\n",
    "    f1_train = f1_score(discrete_y, discrete_y_pred)\n",
    "    mse = mean_squared_error(y_test, y_test_pred)\n",
    "    precision = precision_score(discrete_y_test, discrete_y_test_pred)\n",
    "    recall = recall_score(discrete_y_test, discrete_y_test_pred)\n",
    "    f1 = f1_score(discrete_y_test, discrete_y_test_pred)\n",
    "\n",
    "    scores = {\n",
    "        \"f1_train\": f1_train,\n",
    "        \"mse\": mse,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results[\"rank_test_score\"] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\n",
    "                \"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                    results[\"mean_test_score\"][candidate],\n",
    "                    results[\"std_test_score\"][candidate],\n",
    "                )\n",
    "            )\n",
    "            print(\"Parameters: {0}\".format(results[\"params\"][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_runs(method_dict):\n",
    "    \n",
    "    best_scores = []\n",
    "\n",
    "    for fe_type in fill_methods:\n",
    "        X_train = method_dict[\"X\"][fe_type]\n",
    "        X_test = method_dict[\"X_test\"][fe_type]\n",
    "        y_train = method_dict[\"y\"][fe_type]\n",
    "        y_test = method_dict[\"y_test\"][fe_type]\n",
    "\n",
    "        base_estimator = DecisionTreeRegressor(random_state=0)\n",
    "        param_grid = {\n",
    "            \"max_depth\": [20],#scipy.stats.randint(5, 60),\n",
    "            \"min_samples_leaf\": [1],#scipy.stats.randint(1, 100),\n",
    "            \"criterion\": [\"squared_error\"],\n",
    "        }\n",
    "        # base_estimator = LinearRegression()\n",
    "        # param_grid = {}\n",
    "        sh = GridSearchCV(base_estimator, param_grid, scoring=\"neg_mean_squared_error\").fit(X_train, y_train)\n",
    "        clf = sh.best_estimator_\n",
    "        report(sh.cv_results_)\n",
    "\n",
    "        scores = train_run(X_train, y_train, X_test, y_test, clf=clf)\n",
    "\n",
    "        best_scores.append({\n",
    "            \"type\": fe_type, \"f1_train\": scores[\"f1_train\"],\"mse\": scores[\"mse\"], \"f1\": scores[\"f1\"], \n",
    "            \"precision\": scores[\"precision\"], \"recall\": scores[\"recall\"], \"clf\": clf})\n",
    "    \n",
    "    return best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run ./utility/model_loader.py -i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores = train_runs(method_dict=method_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse = float('inf')\n",
    "best_data = None\n",
    "for score in best_scores:\n",
    "    print(score[\"mse\"])\n",
    "    if score[\"mse\"] < best_mse: \n",
    "        print(score[\"type\"])\n",
    "        best_data_train = pd.concat([method_dict[\"X\"][score[\"type\"]], method_dict[\"train_timestamps\"][score[\"type\"]], method_dict[\"y\"][score[\"type\"]]], axis=1)\n",
    "        best_data_test = pd.concat([method_dict[\"X_test\"][score[\"type\"]], method_dict[\"test_timestamps\"][score[\"type\"]], method_dict[\"y_test\"][score[\"type\"]]], axis=1)\n",
    "        best_data = pd.concat([best_data_train, best_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = best_data.sort_values(by=\"Timestamp\")\n",
    "data_sorted = data_sorted[data_sorted[\"Turbine_ID\"] == \"T06\"]\n",
    "data_sorted.head()\n",
    "# y_pred = best_scores[0][\"clf\"].predict(data_sorted.drop([reg_target_name, \"Turbine_ID\", \"Timestamp\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "ax1.scatter(range(len(y_pred)), y_pred)\n",
    "y_label = data_sorted[\"RUL (Target)\"]\n",
    "ax2.scatter(range(len(y_label)), y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "ax1.hist(y_pred[y_pred!=1], bins=20)\n",
    "ax2.hist(y_label[y_label!=1], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = best_scores[0][\"clf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.concat([X_compounded[\"ffill\"], y_compounded[\"ffill\"]], axis=1)\n",
    "my_sample = my_data[my_data[\"RUL (Target)\"] < 1]\n",
    "prediction = my_clf.predict(my_sample.drop(\"RUL (Target)\", axis=1))\n",
    "actual = my_sample[\"RUL (Target)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prediction, bins=20, log=True)\n",
    "plt.hist(actual, bins=20, log=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('XAI_wind')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "520723b62f68f94512b4b527a2129921bc4d6da9c94363ee0894f78e84b35631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
